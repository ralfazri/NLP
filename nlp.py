# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UALMFto2JJbDhjIIvobzrg6ILltMP1y
"""

#download dataset from kaggle dataset
from google.colab import files
files.upload()
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle
!kaggle datasets download -d hgultekin/bbcnewsarchive

#install kaggle package
!pip install -q kaggle

#unzip file
!mkdir bbcnews
!unzip bbc-news-data.csv.zip -d bbcnews
!ls bbcnews

import pandas as pd
df_bbcnews_nlp = pd.read_csv('bbcnews/bbc-news-data.csv', sep='\t')
df_bbcnews_nlp.head()

#show list columns
df_bbcnews_nlp.columns

df_bbcnews_nlp['category'].unique()

#shape of data
print('Total Row:', df_bbcnews_nlp.shape[0])
print('Total Columns:', df_bbcnews_nlp.shape[1])
df_bbcnews_nlp.shape

df_bbcnews_nlp['category'].value_counts()

#Data Visualization
import matplotlib.pyplot as plt

category = df_bbcnews_nlp['category'].value_counts()
label = category.index
category.plot(kind='pie', labels=label, autopct='%1.1f%%')
plt.title('Category Percentage')
plt.show()

#drop 1 useless column
df_bbcnews_nlp = df_bbcnews_nlp.drop(columns=['filename'])

#check if missing value in data
df_bbcnews_nlp.isnull().any()

#one-hot-encoding
categoryData = pd.get_dummies(df_bbcnews_nlp['category'])
new_df_bbcnews_nlp = pd.concat([df_bbcnews_nlp, categoryData], axis=1)
new_df_bbcnews_nlp = new_df_bbcnews_nlp.drop(columns = 'category')
new_df_bbcnews_nlp.head()

#processing text
import re
from string import punctuation
import nltk

nltk.download("stopwords")
from nltk.corpus import stopwords

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

from nltk.corpus import wordnet as wn
nltk.download('averaged_perceptron_tagger')

def process_text(text):
  #convert string to str
  text = str(text)

  #convert to lower strings
  text = text.lower()


  #remove the url
  url_remove = re.compile(r'(http|ftp|https)://([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?')
  text = re.sub(url_remove,' ',text)

  #remove the punctuation
  text = ''.join([string for string in text if string not in punctuation and not string.isdigit()])

  #remove special characters
  special_character = re.compile(r'[^a-zA-Z]')
  text = re.sub(special_character,' ',text)
  text = text.strip()
  text = text.split(' ')

  #removing all stop words
  text = ' '.join([string for string in text if string not in stopwords.words('english')])

  #removing number
  text = re.sub('[0-9]+','',text)

  #lematization
  lemmatizer = WordNetLemmatizer()
  pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
  text = (' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(text.split())]))

  #remove html
  text = re.sub("(<.*?>)","",text)

  #remove non-ascii and digits
  text=re.sub("(\\W|\\d)"," ",text)

  #remove whitespace
  text = text.strip()

  return text

new_df_bbcnews_nlp.loc[:, ['title']] = new_df_bbcnews_nlp['title'].apply(process_text)
new_df_bbcnews_nlp.loc[:, ['content']] = new_df_bbcnews_nlp['content'].apply(process_text)
new_df_bbcnews_nlp.head()

bbcnews_columns = ['title','content']
label_columns = [column for column in new_df_bbcnews_nlp.columns if column not in bbcnews_columns]

bbcnews = [",".join(item) for item in new_df_bbcnews_nlp[bbcnews_columns].values.astype(str)]
label = new_df_bbcnews_nlp[label_columns].values

from sklearn.model_selection import train_test_split
bbcnews_train, bbcnews_test, label_train, label_test = train_test_split(bbcnews, label, test_size=0.2, random_state=42, shuffle = True)

#Specific Tokenizing
size = 17000
embedding_dim = 64
max_len = 256
trunc_type = "post"
oov = "<OOV>"

#Tokenizing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=size, oov_token=oov)
tokenizer.fit_on_texts(bbcnews_train)

sekuens_train = tokenizer.texts_to_sequences(bbcnews_train)
sekuens_test = tokenizer.texts_to_sequences(bbcnews_test)

padded_train = pad_sequences(sekuens_train, maxlen=max_len, truncating=trunc_type)
padded_test = pad_sequences(sekuens_test, maxlen=max_len, truncating=trunc_type)

#Data Modelling

import tensorflow as tf
model = tf.keras.Sequential([
  tf.keras.layers.Embedding(input_dim=size, output_dim=embedding_dim, input_length=max_len),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),
  tf.keras.layers.GlobalMaxPool1D(),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.95 and logs.get('val_accuracy')>0.95):
      self.model.stop_training = True
      print("\nAccuracy of training set and validation set has reached 95%")
callbacks = myCallback()

num_epochs = 30
history = model.fit(
    padded_train, 
    label_train, 
    epochs=num_epochs,
    validation_data=(padded_test, label_test),
    callbacks=[callbacks], 
    verbose = 2)

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

epochs = range(len(acc))

fig, ax = plt.subplots(figsize=(20,8))
ax.plot(epochs, acc, 'r', label='Training Accuracy')
ax.plot(epochs, val_acc, 'b', label='test Accuracy')
ax.set_title('Accuracy Model')
plt.legend(loc=0)
plt.figure()

plt.show()